\documentclass{article}           % use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                           % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                          % ... or a4paper or a5paper or ... 
%\geometry{landscape}                           % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}                  % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}                           % Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
                                                % TeX will automatically convert eps --> pdf in pdflatex                

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corrolary}{Corrolary}
 
\newcommand{\R}{\mathbb{R}}
\newcommand{\lowerT}[1]{\overrightarrow{#1}}
\newcommand{\cov}{\vartriangleleft}
\newcommand{\Pos}{\mathsf{Pos}}
\newcommand{\Type}{\mathcal{U}}
\newcommand{\Prop}{\mathcal{P}}
\newcommand{\List}[1]{\mathsf{List}\ {#1}}
\newcommand{\map}[2]{\mathsf{map}\ {#1}\ {#2}}
\newcommand{\fun}[2]{\lambda {#1}.\  {#2}}
\newcommand{\nat}{\mathbb{N}}
\newcommand{\suchthat}{\ |\ }
\newcommand{\concat}{\ensuremath{+\!\!\!\!+\,}}
\newcommand{\One}{\mathsf{One}}

\title{Overview of Probability in Coq}
\author{Ben Sherman}
%\date{\today}                                  % Activate to display a given date or no date

\begin{document}
\maketitle

\section{Introduction}

My goal is to develop a framework for reasoning about probability in Coq. I believe this can be useful for a few reasons:
\begin{itemize}
\item To reason about randomized algorithms
\item To reason about systems which operate under uncertain/probabilistic conditions (e.g., a random external environment, or systems which occasionally have random faults)
\item To allow a new sort of probabilistic programming language, where a user first specifies the distribution they wish to understand in a simple way, and then uses a theorem prover to derive efficient (verified) algorithms to sample, compute a PDF, or compute an expectation of the distribution (by composing lemmas of known algorithms/tactics)
\end{itemize}

E.T. Jaynes states \cite{jaynes2003},
\begin{quote}
Probability theory as it is needed in current applications --- the principles for assigning probabilities by logical analysis of incomplete information --- is not present at all in the Kolmogorov system.
\end{quote}

By developing a probability framework in theorem provers, we can close this gap, by connecting the logic to the probability.

\subsection{Foundations}

Reasoning about probability in Coq requires developing the theory of probability within Coq. While one could in principle admit the law of the excluded middle as an axiom, and then more-or-less follow measure theory, I don't think this is the right way to go. First of all, most Coq libraries do not admit the law of the excluded middle, and it is potentially possible that some admit axioms which refute the excluded middle. But more importantly, what isn't constructive ends up being useless anyway; if we used measure theory, we would still need to add many computational principles alongside it just to make it useful. By staying constructive, we eliminate the unnecessary distinction between mathematical proofs and computer programs.

Therefore, there is much to gain by developing a theory of probability which uses only constructive principles. The section \emph{Valuations} provides some technical material explaining one possible approach involving formal topologies and continuous valuations.

Errett Bishop claims that the disconnect between probability theory and probability as it is actually used by practitioners is largely explained by the nonconstructivity of classical probability theory\cite{bishop1973}:

\begin{quote}
One suspects that the majority of pure mathematicians... ignore as much content as they possibly can. If this suspicion seems unjust, pause to consider the modern theory of probability. The probability of an event is commonly taken to be a real number between 0 and 1. One might naively expect that the probabilists would concern themselves with the computation of such real numbers. If so, a quick look at any one of a number of modern texts, for instance the excellent book of Doob, should suffice to disabuse him of that expectation. Fragmentation ensues, because much if not most of the theory is useless to someone who is concerned with actually finding probabilities. He will either develop his own semi-independent theories, or else work with ad hoc techniques and rules of thumb.
\end{quote}

In a constructive framework, a probability distribution on $A$ is a function which (as a special case) takes as input a decidable properties on $A$ and returns a constructive real number (that is, an arbitrarily fine rational approximation) between 0 and 1. 

\subsection{Possible directions}

\subsubsection{Synthesis of probability distributions}

Often times, we wish to characterize probability distributions in ways that aren't exactly constructive. For instance, when rolling a die, the key characterization is that landing on any fact is equally likely; or in other words, a permutation of the die faces preserves the measure. From this description, we may want to synthesize a concrete probability distribution which satisfies this property.

The problem of finding probability distributions which satisfy certain properties can be phrased as a Fiat-style deductive synthesis problem. A contradictory characterization (e.g., ``I am certain the die lands on an even number, and that 1 is more likely that 6'') has no synthesis solution, while underspecified characterizations have many synthesis solutions which might have different properties.

Providing a framework for synthesis in this way would help bridge the gap about which Jaynes complains.

\subsubsection{Probabilistic programming language}

Another potential application is to develop a probabilistic programming language embedded in Coq. Probabilistic programming languages allow users to perform probabilistic inference; essentially, the programmer specifies a prior distribution, and then conditions it based on some observation, and wishes to compute some properties of the posterior distribution. There are many ways to learn information about a probability distribution:

\begin{enumerate}
\item sampling from it
\item evaluating a PDF (i.e., Radon-Nikodym derivative) with respect to a better-known distribution
\item computing expectations or moments
\item closed form symbolic definition
\end{enumerate}

For the most part (I believe), probabilistic programming languages only offer the first option. Because their inference algorithms must be general and fully automatic, they are often far less efficient than inference algorithms that could be manually derived.

A probabilistic programming language embedded in Coq could solve many of the shortcomings of existing probabilistic programming languages. First of all, it allows us to separate the problem of specifying a probability distribution from the problem of computing its properties. Users can specify probability distributions by writing probabilistic programs whose meaning is directly interpreted as a probability distribution.

The user can then use either tactics or higher-order programs to learn about the probability distribution they specify. For instance, they may use a rewrite rule to simplify a sum of Gaussians as a single Gaussian.

We can also give a semantics of samplers, particularly in a nice way so that they form a monad. A sampler for a distribution $\mu$ on the space $A$ using a random source distribution $\mu_R$ on the space of random seeds $R$ is a function $ f : R \to R \times A$ such that
\[
  \mathsf{map}\ f \ \mu_R = \mu_R \times \mu,
\]
where the $\times$ operation represents the product of independent distributions.

Deriving \emph{efficient} samplers can then be phrased as a deductive synthesis problem: to sample from a distribution $\mu$, we want to find a random source distribution $\mu_R$ and a sampling function $f$ which samples $\mu$. Because of the compositionality of probabilistic programs as well as samplers, we can derive samplers in a compositional manner. One can imagine a user deriving sampling algorithms using tactic-based process, where a user can use a user-provided database of efficient samplers which have been proven correct, and then compile a sampler by composing these efficient samplers with pre-existing sampling combinators.

We can imagine a situation in the case where a user may want to derive a PDF for their distribution. This also can be phrased as a problem of deductive synthesis.

\subsubsection{Verification of uncertain systems}

We can use theorem provers such as Coq to verify the logical or absolute correctness of software, as well as systems in a more general sense. However, often software and systems operate under uncertain conditions which are best modeled as random. Viewing probability as extended logic, we can naturally generalize the application of theorem provers to verifying that systems provide some guarantee with at least some probability. 

Note that verifying that a system provides a guarantee with at least some probability is a significantly easier task than computing exactly what that probability is.

For instance, one goal of approximate computing is to design systems which operate well even though their components may be occasionally faulty. We expect distributed systems to provide certain guarantees although some nodes of the systems may be faulty. Network communications involve uncertainty as well.

\section{Valuations}

\subsection{References}

\cite{jones1990}: This is basically the first definition of valuations on topologies, and gives a fairly thorough explanation of them and some in-depth, low-level facts about them.

\cite{maietti2005}: First of all, this paper presents ``join-formal topologies", which are what I use to define measure for formal topologies. They require the base to have a join operation in addition to a meet operation, so that the base is actually a lattice.

Second of all, the result of this paper should be useful to me for defining a topology on the space of valuations itself. The domain of a valuation - a formal topology - should have a topology which is locally compact, and the codomain of a valuation should be an inductively generated formal topology, so the result says that the space of valuations can also be given an inductively generated formal topology. This will be key for showing that valuations form a monad over inductively generated formal topologies, I think!

\cite{escardo2004}: This is a really interesting book. It presents topology ``synthetically"; instead of taking topologies as ``a class of subsets such that..." or even as formal topologies, it presents it essentially as a programming language, where the continuity of functions is expressed by the fact that the function can be written in this programming language.

The way that I think that working with formal topologies in Coq will be made pleasant is by embedding such a programming language in Coq, and showing that formal topologies can serve as a model for synthetic topology.

\cite{vickers1989}: I loaned this from the library. It is a rare resource in that it actually tries to motivate the relevance of locales in a comprehensible way. It is, in this way, very friendly.

\cite{coquand2003}. \cite{sambin2000}.

\subsection{Basic valuations}

In this section, we define a notion of a \emph{basic valuation} on a formal topology, which is a function which assigns each basic open weight in a coherent manner. We then show that a basic valuation can be extended to the standard notion of a continuous valuation on the locale determined by the formal topology.

Let $\R$ denote the real numbers (presented as two-sided Dedekind cuts over propositions, not including points at infinity). Let $\lowerT{\R}$ denote the lower real numbers (presented as one-sided lower Dedekind cuts over propositions, including $+\infty$). Similarly, let $\R^+$ denote the non-negative reals (not including $\infty$) and $\lowerT{\R}^+$ the non-negative lower reals (including $\infty$).

Let $(S, \cov, \bullet, \vee, \Pos)$ be a join-closed formal topology, where $S : \Type$, $\cov : S \to (S \to \Prop) \to \Prop$, $\bullet : S \to S \to S$, $\vee : S \to S \to S$ and $\Pos : S \to \Prop$. Let $\mathcal{A} : (S \to \Prop) \to (S \to \Prop)$ be the saturation operator, defined by
\[
\mathcal{A}(U) = \{ a : S \suchthat a \cov U\}.
\]

\subsection{Join closures}
For an open cover $U : S \to \Prop$, we define its join-closure $\bigvee U$ to be the inductive type with constructors
\begin{gather*}
\frac
{a \in U}
{a \in \bigvee U}
\tag{$\bigvee$-refl}
\\
\frac
{a \in \bigvee U \quad b \bigvee \in U}
{a \vee b \in \bigvee U}.
\tag{$\bigvee$-join}
\end{gather*}

\begin{lemma}
\label{bigvee-cov}
If $U \cov V$, then $\bigvee U \cov V$.
\end{lemma}
\begin{proof}
Suppose $a \in \bigvee U$; either it is in $U$, in which case we immediately know $a \cov V$, or it is of the form $a = b \vee c$, where we know $b \cov V$ and $c \cov V$. Then by the $\vee$-left rule, we know $a \cov V$ as well.
\end{proof}

\begin{corrolary}
For all $U : S \to \Prop$, $U =_\mathcal{A} \bigvee U$.
\end{corrolary}

\begin{lemma}
\label{bigvee-subset}
If $U \subseteq V$, then $\bigvee U \subseteq \bigvee V$.
\end{lemma}

\begin{lemma}
\label{bigvee-distr}
For all $U, V : S \to \Prop$, we have $(\bigvee U) \bullet (\bigvee V) \subseteq \bigvee (U \bullet V)$.
\end{lemma}
\begin{proof}
Follows from the distributivity of $\bullet$ over $\vee$ in $S$. Informally,
\[ 
(\bigvee_i u_i) \bullet (\bigvee_j v_j)
= \bigvee_i \bigvee_j (u_i \bullet v_j),
\]
where the left-hand side is in $(\bigvee U) \bullet (\bigvee V)$ and the right-hand side is in $\bigvee (U \bullet V)$.
\end{proof}

\subsection{Basic valuations}

Then a \emph{basic valuation} on $(S, \cov, \bullet, \vee, \Pos)$ is a function $\mu : S \to \lowerT{\R}^+$ such that the following propositions hold:

\begin{gather*}
\frac
{a \cov U}
{\mu(a) \le \mu_\infty(U) }
\tag{monotone}
\\
\mu(a) + \mu(b) = \mu(a \bullet b) + \mu(a \vee b)
\tag{modular}
\end{gather*}
where we define the extension of $\mu$ to open covers $\mu_\infty : (S \to \Prop) \to \lowerT{\R}^+$ by
\[
\mu_\infty(U) = \sup \{ \mu(a) \suchthat a \in \bigvee U \},
\]
and we define the supremum of the empty set as 0.

In the remainder of this section, assume $\mu$ is a basic valuation.

\begin{lemma}
We know that $\mu$ is \emph{strict}, meaning that
\[
\frac
{\Pos(a) \to \mu(a) = 0}
{\mu(a) = 0}.
\tag{strict}
\]
\end{lemma}
\begin{proof}
Suppose we have $a : S$ and know that $\Pos(a) \to \mu(a) = 0$. We know $a \cov \{ a \suchthat \Pos(a) \}$, so by monotonicity $\mu(a) \le \sup_{\{a \suchthat \Pos(a)\}} \mu(a) = 0$.
\end{proof}

\begin{lemma}
If $U \cov V$, then $\mu_\infty(U) \le \mu_\infty(V)$.
\end{lemma}
\begin{proof}
Suppose we lower-bound $\mu_\infty(U)$ with $a \in \bigvee U$. Since $U \cov V$, by lemma \ref{bigvee-cov} we have $\bigvee U \cov V$ and thus $a \cov V$, so by monotonicity of $\mu$ we get $\mu(a) \le \mu_\infty(V)$, thus giving the same lower bound for $\mu_\infty(V)$.
\end{proof}

\begin{corrolary}
If $U \subseteq V$, then $\mu_\infty(U) \le \mu_\infty(V)$. If $U =_\mathcal{A} V$, then $\mu_\infty(U) = \mu_\infty(V)$. In particular, $\mu_\infty(\mathcal{A}U) = \mu_\infty(U)$. Additionally,
\[
\mu_\infty(U) = \sup \{\mu(a) \suchthat a \cov U \}.
\]
\end{corrolary}

\begin{lemma}
We have, for a singleton basic cover, $\mu_\infty(\fun{z}{z = a}) = \mu(a)$.
\end{lemma}

\begin{lemma}
For all $a : S$ and $b : S$, $\mu(a \bullet b) \le \mu(a)$.
\end{lemma}
\begin{proof}
Since $a \bullet b \cov a$, we know $\mu(a \bullet b) \le \mu_\infty(\fun{z}{z = a}) = \mu(a)$.
\end{proof}

\begin{lemma}
$\mu_\infty$ is continuous; that is, for every join-semilattice $L$ and directed set $f : L \to (S \to \Prop)$ (where $i \le j$ implies $f(i) \subseteq f(j)$), we have
\[
\mu_\infty\left(\bigcup_{l : L} f(l)\right) = \sup_{l : L} \mu_\infty(f(l)).
\]
\end{lemma}
\begin{proof}
It is easy that the right-hand side is no greater than the left-hand side, because for each $l : L$, we know $f(l) \subseteq \cup_{l' : L} f(l')$. To see that the left-hand side is no greater than the right-hand side, suppose we lower-bound the left-hand side by finding some $a : S$ such that $a \in \bigvee \cup_{l : L} f(l)$. We claim that there is some $l_a$ such that $a \in \bigvee f(l_a)$, and therefore we can lower-bound the right side. 

We reason by induction on the membership of $a$ in $\bigvee \cup_{l : L} f(l)$. If $a \in \cup_{l : L} f(l)$, then directly we have some $l_a$ such that $a \in f(l_a) \subseteq \bigvee f(l_a)$. In the inductive case, we have $a = b \vee c$, where there are $l_b$ and $l_c$ such that $b \in \bigvee f(l_b)$ and $c \in \bigvee f(l_c)$. Then take $l_a = \max(l_b, l_c)$. Since $\bigvee f(l_b) \subseteq \bigvee f(l_a)$, and likewise for $l_c$, we know that $b, c \in \bigvee f(l_a)$. Therefore $a \in \bigvee f(l_a)$.
\end{proof}


\begin{lemma}
$\mu_\infty$ is modular; that is, for every $U, V : S \to \Prop$, we have
\[
\mu_\infty(U) + \mu_\infty(V) = \mu_\infty(U \bullet V) + \mu_\infty(U \cup V),
\]
where $U \cup V$ is just the union (or disjunction) of predicates.
\end{lemma}
\begin{proof}
First, we see that lower-bounding the left-hand side gives a lower bound to the right-hand side: suppose we have $a, b : S$ where $a \in \bigvee U$ and $b \in \bigvee V$. Since by lemma \ref{bigvee-distr} we have $(\bigvee U) \bullet (\bigvee V) \subseteq \bigvee (U \bullet V)$, we can lower-bound the right-hand side with $a \bullet b \in \bigvee (U \bullet V)$ and $a \vee b \in \bigvee (U \cup V)$; this bound is the same due to the modularity property for $\mu$.

It remains to prove that lower-bounding the right-hand side gives a lower bound to the left-hand side. Suppose we have $x, y : S$ where $x \in \bigvee (U \bullet V)$ and $y \in \bigvee (U \cup V)$. We can find $y_U \in \bigvee U$ and $y_V \in \bigvee V$ such that $y = y_U \vee y_V$.

We claim that there exists $x_U \in \bigvee U$ and $x_V \in \bigvee V$ such that $x \in \bigvee (U \bullet V)$ satisfies $x \le x_U \bullet x_V$. Informally, this follows from the fact that given some $\bigvee_i u_i \bullet v_i \in \bigvee (U \bullet V)$, we have 
\[
\bigvee_i u_i \bullet v_i \le \left( \bigvee_i u_i \right) \bullet \left( \bigvee_j v_j \right),
\]
where $\bigvee_i u_i \in \bigvee U$ and $\bigvee_j v_j \in \bigvee V$.

Then we get
\begin{align*}
\mu(x_U \vee y_U) + \mu(x_V \vee y_V)
  &= \mu((x_U \vee y_U) \bullet (x_V \vee y_V)) + \mu((x_U \vee y_U) \vee (x_V \vee y_V))
  \tag{modular}
  \\ &\ge \mu(x_U \bullet x_V) + \mu(y_U \vee y_V)
  \\ &\ge \mu(x) + \mu(y). \tag{monotone}
\end{align*}
Since $x_U \vee y_U \in \bigvee U$ and $x_V \vee y_V \in \bigvee V$, this gives a lower bound to the left-hand side.
\end{proof}

\begin{theorem}
$\mu_\infty$ is a continuous valuation.
\end{theorem}
 
\begin{proof}
Strictness and monotonicity are obvious, and continuity and modularity follow from lemmas above.
\end{proof}

\subsection{Producing join-closed formal topologies}

It may seem a difficult requirement to generate a join-closed formal topology. In fact, there is a construction to produce a join-closed formal topology given in Lemma 2.13 of Maietti et. al.'s \emph{Predicative exponentiation of locally compact formal topologies over inductively generated ones}. Note that this construction, where the basic opens go from being of type $S$ to $\List{S}$, preserves being a set (inductive generation): if $S$ is a set, then so is $\List{S}$.

Accordingly, one can attempt to see what conditions suffice for a valuation defined on a formal topology that would ensure that the derived valuation for the topology, by using the above-mentioned construction, would be a basic valuation (in particular, the difficulty is in satisfying the modularity condition). We could na\"ively translate the modularity back, but perhaps there is a condition that is easier to prove but still implies modularity.


If we do only start with formal topologies $T$ which are not join-closed, then it seems that our valuation must output real numbers $\R^+$ rather than simply lower reals. Suppose we have a function $\mu : T \to \R^+$ and want to extend it to the join-closure encoded by $\List{T}$. Then, the definition $\mu_\vee : \List{T} \to \R^+$ for the elements generated by the join-closure is uniquely determined according to the recursive definition:

\begin{align*}
\mu_\vee([]) &= 0
\\ \mu_\vee(x :: xs) &= \mu(x) + \mu_\vee(xs) - \mu_\vee(x \bullet xs),
\end{align*}
where $x \bullet xs$ is shorthand for $\map{(\fun{z}{x \bullet z})}{xs}$. The subtraction here shows us that $\mu$ must output a two-sided real number rather than a one-sided one. We note that the definition of $\mu_\vee$ is well-founded since the length of the list will decreases by 1 on each recursive call.

Note that $\mu_\vee$ in fact always outputs real numbers, rather than only lower real numbers. This shows that the construction is limited in its scope; not every basic valuation can be defined in this manner. However, it also demonstrates that the class of basic valuations which output real numbers as opposed to lower real numbers is an interesting one; we will call these \emph{real-valued basic valuations}.

\subsection{Products}

In general, the product of two join-formal topologies is not necessarily a join-formal topology. More concretely, given $S$ and $T$ join-formal topologies, and $s_1, s_2 : S$ and $t_1, t_2 : T$, then there is not necessarily already an element representing $(s_1, t_1) \vee (s_2, t_2)$.

More importantly, this implies that defining the product of two basic valuations is not necessarily straightforward, as it is unclear how to assign a lower-real-valued measure to an open such as $(s_1, t_1) \vee (s_2, t_2)$. However, we can easily define the product of two real-valued basic valuations $\mu_S : S \to \R^+$ and $\mu_T : T \to \R^+$ by first defining $\mu : S \times T \to \R^+$ by
\[
\mu(s, t) = \mu_S(s) \mu_T(t),
\]
and then using the join-closure procedure described in the previous section, so that for example we have
\[
\mu((s_1, t_1) \vee (s_2, t_2)) = \mu(s_1, t_1) + \mu(s_2, t_2) - \mu(s_1 \bullet s_2, t_1 \bullet t_2).
\]

\cite{vickers2011} defines products for lower-real-valued valuations over locales by using integration, i.e.,
\[
\mu_{ST} = a \leftarrow \mu_S \ ; \ \map{(\fun{b}{(a, b))}}{\mu_T},
\]
as well as the mirror image $\mu_{TS}$ and shows that they are equivalent in an analog of Fubini theorem. However, the usually Fubini theorem equates these two definitions to a ``standard", symmetric definition of product measures, whereas this sort of definition is conspicuously absent in \cite{vickers2011}, leading me to think that such a definition is not necessarily straightforward, and possibly explaining the difficulty of doing the same thing for formal topologies.

\section{Probability}

Recall that if $\mu$ is a basic valuation, then $\mu_\infty$ is a valuation. In this section, we will ignore formal topology and just work with locales directly, so let $A$ be a locale and $\mu$ be a valuation. If we add the additional requirement that $\mu(\top) = 1$, then we can call $\mu$ a probability distribution. It turns out that this simple requirement adds a lot of structure. For every probability distribution $\mu : A \to \underline{[0,1]}$ defined on the open sets identified with a locale $A$, there is a probability distribution $\mu^* : A \to \overline{[0,1]}$ defined by
\[
\mu^*(a) = 1 - \mu(a)
\]
on the closed sets identified with the locale $A$, where a closed set $a : A$ here is interpreted as the complement of the open set that it usually represents as a locale.

We can view $\mu$ as a measure on open sets which we continue to give lower bounds, increasing from 0, by finding additional ways to verify the open set, while $\mu^*$ is a measure on closed sets that we can upper bounds, decreasing from 1, by finding additional ways to refute the closed set.

\begin{lemma}
\label{comp-real}
Every complemented member of the frame, that is, every clopen, has a measure which is in fact a real number.
\end{lemma}
\begin{proof}
Let $a : A$ have a complement $\bar{a} : A$. Then by modularity, 
\[
\mu(a) + \mu(\bar{a}) = \mu(a \vee \bar{a}) + \mu(a \wedge \bar{a}) = \mu(\top) + \mu(\bot) = 1 + 0 = 1
\]
Since $\mu(a) + \mu(\bar{a}) = 1$, this means that for every $q : \mathbb{Q}$ such that $q < 1$, we can find $x, y : \mathbb{Q}$ such that $x + y = q$ and $x < \mu(a) < 1 - y$ and $y < \mu(\bar{a}) < 1 - x$. Therefore, if we want to locate $\mu(a)$ to within $\varepsilon$, we simply take $q = 1 - \varepsilon$, giving $x < \mu(a) < x + \varepsilon$.
\end{proof}

\subsection{Conditional probability}

Given $a, b : A$, we say that $\mu(a \suchthat b) : \underline{[0,1]}$ is a conditional probability when
\[
\mu(a \wedge b) = \mu(a \suchthat b) \mu(b).
\]

If $\mu(b) = 0$, then $\mu(a \wedge b) = 0$ as well, and any value will suffice for the conditional probability $\mu(a \suchthat b)$. If we know that $\mu(b) > 0$, then reasoning classically, there is a unique solution for $\mu(a \suchthat b)$ given by
\[
\mu(a \suchthat b) = \frac{\mu(a \wedge b)}{\mu(b)}.
\]

However, is not valid constructively. If we want to give $\mu(a \suchthat b)$ a lower bound, we must give $\mu(a \wedge b)$ a lower bound and $\mu(b)$ an \emph{upper} bound. That is, if we view upper real numbers and lower real numbers as having opposite polarities, then division requires the divisor to have the opposite polarity of the dividend and the result.

Intuitively, if we want to affirm that it is likely to affirm $a$ given that we affirm $b$, we must affirm the worlds where we affirm both $a$ and $b$, and refute the worlds where we refute $b$. Therefore, in order to be able to compute $\mu(a \suchthat b)$ in general, we require that $b$ is clopen --- affirmable and refutable --- so that by lemma \ref{comp-real} we can divide by $\mu(b)$.

\subsection{Simple functions and integration}

Remark: we can add valuations and multiply them by scalars (where the scalars are also lower real numbers).

We define an inductive family $\mathsf{Simple} : \mathbf{Loc} \to \Type$ according to

\begin{gather*}
\frac
{q : \mathbb{Q}^+ \qquad a : A}
{\mathsf{Ind}\ q\ a : \mathsf{Simple}\ A}
\\
\frac
{f : \mathsf{Simple}\ A \qquad g : \mathsf{Simple}\ A }
{\mathsf{Add}\ f\ g : \mathsf{Simple}\ A}.
\end{gather*}

We define the integral of simple functions by
\begin{align*}
   \int_A (\mathsf{Ind}\ q\ a) d\mu &= q \mu(a)
\\ \int_A (\mathsf{Add}\ f\ g) d\mu &= \int_A f d\mu + \int_A g d\mu.
\end{align*}

Each $f : \mathsf{Simple}\ A$ corresponds to a continuous map $\mathsf{eval}\ f: \mathcal{C}(A, \underline{\R}^+)$ in the obvious way.

Given two continuous maps $f, g : \mathcal{C}(A, \underline{\R}^+)$, we say that $f \le g$ if for every $q \in \mathbb{Q}^+$
\[
\{ x \in A \suchthat q < g(x) \} \subseteq \{ x \in A \suchthat q < f(x) \}.
\]
Note that the above condition is written in a very suggestive (misleading) notation, indicating some notion of pointwiseness, that's not really the case. This is a sort of ``horizontal'' definition of when $f \le g$ rather than the more standard ``vertical'' one: we mark a horizontal line in the range, and check that the subset of the domain whose image under $g$ falls below line is included in the subset of the domain whose image under $f$ falls below the line. Normally, we see a ``vertical'' definition, where we consider each point $x$ in the domain, and check that $f(x) \le g(x)$ at that point. But in fact the ``horizontal'' definition allows us to remain pointless.

Then we define the integral of a continuous function as
\[
\int_A f d\mu = \sup \left\{ \int_A g d\mu \suchthat g : \mathsf{Simple}\ A, \mathsf{eval}\ g \le f \right\}.
\]

\subsection{Suprema and fixpoints}
We can take the supremum of a directed set of valuations, and that this in turn allows us to describe fixpoints. This allows definition of the geometric distribution, for instance, as a fixpoint of a coin-flipping process.

\section{Examples}

The simplest space is that with a single point, which we will call $\One$. It has no non-trivial open sets. We define its formal topology with the inductive types $S_\One, \cov_\One, \ltimes_\One$ generated by
\begin{gather*}
\top : S_\One
\\ \top \cov_\One \top
\\ \top \ltimes_\One \top
\end{gather*}

It should come as no surprise that there is only a single probability distribution on $\One$, which puts all its mass on the only point that there is:
\[
\mu_\One(U) = \chi(\top \in U).
\]

Perhaps the next simplest space the SierpÃ­nski space $\Sigma$. This space has two points, $\top_\Sigma$ and $\bot_\Sigma$, where the singleton set $\{\top_\Sigma\}$ is open while the singleton set $\{ \bot_\Sigma \}$ is closed. These sets are the only non-trivial open and closed sets, respectively. Since we're working with formal topology, however, we don't ever need to specify the points directly. We use $S_\Sigma, \cov_\Sigma, \ltimes_\Sigma$ generated by
\begin{gather*}
S_\Sigma = 0_\Sigma \quad | \quad 1_\Sigma
\\ 0_\Sigma \cov_\Sigma 1_\Sigma
\\ 1_\Sigma \ltimes_\Sigma 1_\Sigma
\end{gather*}

Here, we should have a bijection between lower reals $r : \underline{[0,1]}$ and probability distributions on $\Sigma$, given by the map $\mu_\Sigma : \underline{[0,1]} \to S_\Sigma \to \underline{[0,1]}$ defined as
\begin{align*}
    \mu(r)(0_\Sigma) &= r
\\  \mu(r)(1_\Sigma) &= 1.
\end{align*}

This means that we observe the open point $\top_\Sigma$ with probability $r$ and refute the closed point $\bot_\Sigma$ with probability $1 - r$.

Given formal topologies $S$ and $T$, a continuous relation $F : S \to T \to \Prop$ and a probability distribution $\mu : A \to \underline{[0,1]}$, we can produce a new probability distribution $\map{f}{\mu} : B \to \underline{[0,1]}$ given by
\[
\map{f}{\mu}(b) = \mu_\infty(\{ s : S \suchthat s\ F\ b \}).
\]

Recall that we can define a point $x$ in the formal topology $S$ as a continuous relation $x : \One \to S \to \Prop$. Then the Dirac delta of this distribution, $\delta_x : S \to \underline{[0,1]}$ is simply given by $\map{x}{\mu_\One}$.

\section{Future plans}

Here I outline what still must be done, both in terms of what is not yet formalized in Coq, and what is not yet clear to me even outside of Coq:

\begin{enumerate}
\item Show that every continuous function from $A \to \underline{\mathbb{R}}^+$ is equivalent to the supremum of simple functions. This is done in \cite{jones1990}, and the argument there should work here as well.
\item Make it easier to deal with formal topologies in Coq and define an embedded programming language that allows us to deal with them. I'm pretty sure that they're effectively a pure programming language that lacks higher-order functions. The topologies we have should have the following structure:
  \begin{itemize}
  \item All finite products, including the 0-ary product, which is the 1-point space, which are associative
  \item The diagonal function, i.e., the continuous map $A \to A \times A$ for every topological space $A$
  \item There's an operation, I don't know what it's called, but if we have $f : A \to X$ and $g : B \to Y$, we get $f \times g : A \times B \to X \times Y$
  \end{itemize}
  As an example, consider the topological statement that if $X$ is compact and $F \subset X$ is closed, then $F$ is compact\cite{escardo2004}. This is realized by the function $\forall_F : \Sigma^X \to \Sigma$ defined by
\[
\forall_F(p) = \forall_X(\lambda x. \chi_{X \setminus F}(x) \vee p(x)).
\]
To ``compile'' this function, we take the following composition of functions
\[
\Sigma^X \times X
\xrightarrow{\mathrm{id} \times \Delta}
\Sigma^X \times (X \times X)
\xrightarrow{\cong}
(\Sigma^X \times X) \times X
\xrightarrow{\varepsilon \times \chi_{X \setminus F}}
\Sigma \times \Sigma
\xrightarrow{\vee}
\Sigma,
\]
which gives us a function of $\Sigma^X \times X \to \Sigma$, which we (using the fact that we know that we can take exponentials of $X$) curry to $\Sigma^X \to \Sigma^X$, which we then compose with $\forall_X : \Sigma^X \to \Sigma$.

\item Show that the functor $\mathsf{Prob} : \mathbf{IgTop} \to \mathbf{IgTop}$ is monadic, where $\mathbf{IgTop}$ is the category of inductively generated formal topologies. This should be possible because a probability distribution on a space $A$ is a function from the opens of $A$ to $\underline{\mathbb{R}}^+$, and the topology on the opens of $A$ should be locally compact (since we give it the Scott topology/specialization topology), and then we follow the argument in \cite{maietti2005}.

This will allow people to describe probability distributions in the familiar monadic style.

\item Show that Fubini theorem holds. It is proved in \cite{vickers2011}. This is important for reasoning about probability distributions: If I draw randomly from $\mu : \mathsf{Prob}\ A$ and then randomly from $\nu : \mathsf{Prob}\ B$ and put them together, it's the same as if I did it the other way around.

\item Think about connecting the world of Coq to the world of topological spaces. Since there are topological models for (at least) simply-typed lambda calculus, we should be able to have an Ltac that converts (at least some simple) Coq functions to their denotations as continuous maps in a topological model. 

\item Having to work with join-closed formal topologies is quite onerous, because the join-closed formal topologies are much larger than standard formal topologies. Particular, one might wish that they could prove the modularity condition for elements of a formal topology, and have that imply the modularity of the join-closure of that formal topology. It intuitively makes sense that this might be the case.

\item Embed metric spaces into formal topologies using the method in \cite{palmgren2007}, and connect these definitions to the definitions of metric completions and metric spaces in CoRN\cite{oconnor2008}. This will allow us to define the familiar distributions on real numbers, such as the normal distribution, using the results already available in CoRN. 

\end{enumerate}

\bibliographystyle{alpha}
\bibliography{Overview}

\end{document}









\section{Trash dump}
We first extend $\mu$ to an operation on finite covers $\mu_\omega : \List{S} \to \R$ according to the recursive definition
\begin{align*}
\mu_\omega([]) &= 0
\\
\mu_\omega(a :: as) &= \mu(a) - \mu_\omega(a \bullet as) + \mu_\omega(as),
\end{align*}
where $a \bullet as$ is written as shorthand for $\map{(\fun{x}{x \bullet a})}{as}$. We note that the definition of $\mu_\omega$ can be proved well-founded by induction on the length of the list. 

We then extend $\mu$ to an operation over infinite covers $\mu_\infty : (S \to \Prop) \to \lowerT{\R}^+$ by
\[
\mu_\infty(U) = \sup \{ \mu_\omega(as) \suchthat \forall a \in as, U a \}.
\]
We note that $\mu_\infty(U)$ must be non-negative, since it is a supremum over quantities including $\mu_\omega([]) = 0$.

\begin{lemma}
For all $a : S$, $\mu_\omega([a]) = \mu(a)$.
\end{lemma}

\begin{lemma}
For all $a : S$ and $xs : \List{S}$, if $a \in xs$, then $\mu_\omega(a :: xs) = \mu_\omega(xs)$.
\end{lemma}

\begin{lemma}
For all $xs, ys : \List{S}$, $\mu_\omega(xs \concat ys) \le \mu_\omega(xs) + \mu_\omega(ys)$.
\end{lemma}

\begin{lemma}
For all $a : S$ and $xs : \List{S}$, if $a \in xs$, then $\mu_\infty(\fun{z}{z \in (a :: xs)}) = \mu_\infty(\fun{z}{z \in xs})$.
\end{lemma}
\begin{proof}
The two subsets have the same extent.
\end{proof}

\begin{lemma}
If $\mu$ is a basic valuation, for all $z : S$ and $zs : \List{S}$, $\mu_\omega(z \bullet zs) \le \mu_\omega(zs)$.
\end{lemma}
\begin{proof}
By induction on the maximum length of $zs$. Base case is obvious. In the inductive case, we know 
\[ \mu_\omega(z \bullet zs) \le \mu_\omega(zs) \]
for every $z$ and every $zs$ of length no greater than $n$
and must prove the following for $xs$ of length $n$:
\begin{align*}
\mu_\omega(a \bullet x :: a \bullet xs) &\le \mu_\omega(x :: xs)
\\
\mu(a \bullet x) - \mu_\omega(x \bullet (a \bullet xs)) + \mu_\omega(a \bullet xs) 
  &\le \mu(x) - \mu_\omega (x \bullet xs) + \mu_\omega(xs)
\\
\mu(a \bullet x) &\le \mu(x).
\end{align*}
We stepped from the second to third line from two applications of the inductive hypothesis (with $z = x, zs = a \bullet xs$ and $z = x, zs = xs$). Finally, we know $\mu(a \bullet x) \le \mu(x)$ by the previous lemma.
\end{proof}

\begin{lemma}
If $\mu$ is a basic valuation, for all $z : S$ and $zs : \List{S}$, $\mu_\omega(z \bullet zs) \le \mu(z)$.
\end{lemma}
\begin{proof}
By induction on the maximum length of $zs$. Base case is obvious. In the inductive case, we know 
\[ \mu_\omega(z \bullet zs) \le \mu(z) \]
for every $z$ and every $zs$ of length no greater than $n$
and must prove the following for $xs$ of length $n$:
\begin{align*}
\mu_\omega(a \bullet x :: a \bullet xs) &\le \mu(a)
\\
\mu(a \bullet x) - \mu_\omega((a \bullet x) \bullet xs) + \mu_\omega(a \bullet xs) 
  &\le \mu(a)
\\
\mu(a \bullet x) &\le \mu(a).
\end{align*}
We stepped from the second to third line using the inductive hypothesis (with $z = a \bullet x, zs = xs$). Finally, we know $\mu(a \bullet x) \le \mu(a)$ by a previous lemma.
\end{proof}

\begin{corrolary}
If $\mu$ is a basic valuation, for all $z : S$ and $zs : \List{S}$, $\mu_\omega(zs) \le \mu_\omega(z :: zs)$. Generalizing this, for any $xs, ys : \List{S}$, $\mu_\omega(xs) \le \mu_\omega(xs \concat ys)$. Additionally, this implies $0 \le \mu_\omega(ys)$.
\end{corrolary}

\begin{theorem}
If $\mu$ is a basic valuation, then for all $xs : \List{S}$, $\mu_\omega(xs) = \mu_\infty(\fun{z}{z \in xs})$.
\end{theorem}
\begin{proof}
The definition of $\mu_\omega$, and the fact that $\bullet$ is a commutative monoid, implies that if $xs$ and $ys$ have the same elements, then $\mu_\omega(xs) = \mu_\omega(ys)$. This, combined with the corrolary above which tells us that $\mu_\omega$ is monotonic with respect to list concatenation, tells us that $\mu_\omega$ is monotonic with respect to list membership, i.e., if $\forall z, z \in xs \to z \in ys$, then $\mu_\omega(xs) \le \mu_\omega(ys)$.
\end{proof}

\begin{lemma}
If $\mu$ is a basic valuation, then $\mu_\omega$ is modular; that is, for every $xs, ys : \List{S}$, we have
\[
\mu_\omega(xs) + \mu_\omega(ys) = \mu_\omega(xs \bullet ys) + \mu_\omega(xs \concat ys),
\]
using the recursive definition
\begin{align*}
[] \bullet ys &= []
\\ (x :: xs) \bullet ys &= x \bullet ys \concat xs \bullet ys
\end{align*}
\end{lemma}
\begin{proof}
By induction on the structure of $xs$. The base case is obvious. In the inductive case, we must prove
\begin{align*}
\mu(x) - \mu_\omega(x \bullet xs) + \mu_\omega(xs) + \mu_\omega(ys)
  &= \mu_\omega((x :: xs) \bullet ys) 
  + \mu(x) - \mu_\omega(x \bullet (xs \concat ys)) + \mu_\omega(xs \concat ys)
\\
     \mu_\omega(xs \bullet ys) - \mu_\omega(x \bullet xs)
  &= \mu_\omega((x :: xs) \bullet ys) - \mu_\omega(x \bullet (xs \concat ys))
\end{align*}
We now do structural induction on $ys$. The base case is again, obvious, and in the inductive case we get
\begin{align*}
     \mu_\omega(xs \bullet (y :: ys)) - \mu_\omega(x \bullet xs)
  &= \mu_\omega((x :: xs) \bullet (y :: ys)) - \mu_\omega(x \bullet (xs \concat y :: ys))
\\
     \mu_\omega(xs \bullet (y :: ys)) - \mu_\omega(x \bullet xs)
  &= \mu_\omega(x \bullet y :: xs \bullet y \concat x \bullet ys \concat xs \bullet ys) - \mu_\omega(x \bullet y :: x \bullet (xs \concat ys))
\\
     \mu_\omega(xs \bullet (y :: ys)) - \mu_\omega(x \bullet xs)
  &= - \mu_\omega((x \bullet y) \bullet (xs \concat ys \concat xs \bullet ys)) 
     + \mu_\omega(xs \bullet y \concat x \bullet ys \concat xs \bullet ys) 
  \\  &+ \mu_\omega(x \bullet y \bullet (xs \concat ys))
    - \mu_\omega(x \bullet (xs \concat ys))
\\
     \mu_\omega(xs \bullet (y :: ys)) - \mu_\omega(xs \bullet ys)
  &= - \mu_\omega((x \bullet y) \bullet (xs \concat ys \concat xs \bullet ys)) 
     + \mu_\omega(xs \bullet y \concat x \bullet ys \concat xs \bullet ys) 
  \\  &+ \mu_\omega(x \bullet y \bullet (xs \concat ys))
    - \mu_\omega((x :: xs) \bullet ys)
\\
     \mu_\omega(xs \bullet (y :: ys)) - \mu_\omega(xs \bullet ys)
  &= \mu_\omega(xs \bullet y \concat x \bullet ys \concat xs \bullet ys) 
    - \mu_\omega((x :: xs) \bullet ys)
\end{align*}

I am lost and give up. It shouldn't have been this hard...
\end{proof}
