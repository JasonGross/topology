\subsection{Introduction and motivating examples}

The purpose of this note is to explain how the notion of pattern matching in functional programming can be generalized to programming with continuous spaces, where the patterns should be allowed to overlap. This notion of pattern matching highlights the centrality of open covers, locality, and non-determinism for programming continuous functions.

The idea arose from assimilating two example programs. One is a toy example that I now like to call ``Buridan's autonomous car.'' An autonomous car is approaching a yellow light and must decide whether to stop for the light or to proceed past the intersection. We imagine that the car's state is parameterized only by its distance from the light, a real number ($\R$), and that its output should be Boolean-valued ($\bool$), indicating whether or not to brake. If the car is very far backwards, then the car should certainly brake, but if it is very far forwards, it should certainly proceed through the intersection, so the decision should not be a constant function. However, we notice that since the constant functions are the only possible continuous maps from $\R$ to $\bool$ (in topological jargon, $\R$ is connected), we must do something else.

That ``something else'' is to allow non-determinism in the output decision. If it is both safe to stop as well as safe to go, it's alright to make either decision, even if one sometimes makes different decisions based on the \emph{same exact} input value. There is space $\PLower^+(\bool)$ of non-empty subsets of $\bool$ which should be the return value of the braking decision, rather than $\bool$ itself. Then we can write the following function to specify the braking behavior:
\begin{align*}
\mathsf{brake?} &: \R \to \PLower^+(\bool)
\\ \mathsf{brake?}(s) &\triangleq \mathsf{cases}(s)
\begin{cases}
\cdot > - 1
  \quad &\Longrightarrow \quad
  \{ \mathsf{false} \}
\\
\cdot < 0
  \quad &\Longrightarrow \quad
  \{ \mathsf{true} \}.
\end{cases}
\end{align*}

Note that while the two cases do indeed cover the entire space $\R$, they overlap. The computational interpretation is that the scrutinee of a $\mathsf{cases}$ expression is allowed to choose any case which it satisfies and then exhibit the behavior in that branch. Therefore, the result behavior is the union of the behaviors of all possible branches. If an input satisfies two cases, such as $-0.5 : \R$ in the above definition, then its ``behavior'' is the ``maximum'' of $\{ \mathsf{true} \}$ and $\{ \mathsf{false} \}$, which in this case is $\{ \mathsf{true}, \mathsf{false} \}$. It is not always the case it's possible to take a union of behaviors of several programs, as it is here (in fact, this union exists for any two expressions of type $\PLower(A)$ for any $A$). In fact, as long as one can produce a maximum of two branches, when restricted to the intersection of their domains, then the $\mathsf{cases}$ expression will be well-defined.

The other example is the definition of multiplication of real numbers. It's quite different from the previous example: the pattern match has infinitely many cases (all overlapping each other), and produces a deterministic function. I have defined the real numbers $\R$ using a generic construction of completion of metric spaces, taking the space $\R$ as the metric completion of the set $\rat$. The main construction that I have allows Lipschitz functions defined on metric ``sets'' (e.g., the set $\rat$ with its relevant metric) to be lifted to their metric completions. This is quite general, but already multiplication of real numbers is not Lipschitz, and therefore cannot be defined as the extension of multiplication of rational numbers. However, multiplication is \emph{locally} Lipschitz; if one restricts one of the factors to have absolute value bounded by $L$, then $L$ is a satisfactory Lipschitz constant for the multiplication. Therefore, it is possible to define multiplication with the overlapping pattern match
\begin{align*}
\times &: \R \times \R \to \R
\\ x \times y &\triangleq
\mathsf{cases}(x)
\begin{cases}
[L : \rat^+] \quad x' \models -L < \cdot < L \quad \Longrightarrow \quad \mathsf{scale}_L(x', y)
\end{cases},
\end{align*}
where we have the family of functions $\mathsf{scale}_L : \{ \R \suchthat -L < \cdot < L \} \times \R \to \R$ \footnote{For a space $A$ and open $U : \Open{A}$, the notation $\{ A \suchthat U \}$ indicates the open subspace $U$ of $A$.}
for $L : \rat^+$ which are directly defined as the extension of Lipschitz functions on the rationals. In this definition, notice that we ``bind'' the pattern variable $x'$ as a variable of type $\{ \R \suchthat -L < \cdot < L \}$.

To prove that the above definition is in fact valid, we must show that the cases suffice to cover $\R$, and that any two branches have a ``maximum'' on their region of overlap. The former should be clear, and we should be able to prove that branches always \emph{agree} exactly on their region of overlap, so that maximum exists: it's the same as either branch. These proofs confirm the validity of the above definition and provide computational content so that the function can be run.

\subsection{Patterns using sites of the gros topos}

Let's get into the details, and in particular, we will generalize the notion of open covers to sites, which make for more convenient pattern matching.

First, some preliminary notions. For any two spaces $X$ and $Y$, let $X \cong Y$ indicate that the spaces $X$ and $Y$ are homeomorphic, meaning that we have exhibited continuous maps $f : X \to Y$ and $g : Y \to X$ such that $g \circ f = \mathsf{id}_X$ as well as $f \circ g = \mathsf{id}_Y$.
We call a map $f : A \to B$ is an \emph{open embedding} if $f(A)$ is open and $A \cong f(A)$. The notation $f : A \hookto B$ indicates that the continuous map $f : A \to B$ is an open embedding.

The gros topos over \textbf{Top}, the category of topological spaces and their continuous maps, is a Grothendieck topos whose site is \textbf{Top} endowed with the coverage where a collection of open embeddings $\left( f_i : A_i \hookto B \right)_{i : I}$ is considered a cover if
\[
\bigcup_{i : I} f_i(A_i) = B.
\]

In the case where a cover $\left( f_i : A_i \hookto B \right)_{i : I}$ is \emph{disjoint}, i.e., if $i \ne j$, then $f_i(A_i) \cap f_j(A_j)$ is empty, then we recover the ordinary notion of pattern matching. For instance, for two spaces $L, R : \Space$, we have the disjoint covering (often called a \emph{partition})
\[
\{ \mathsf{inl} : L \hookto L + R \,,\, \mathsf{inr} : R \hookto L + R \},
\]
and accordingly we define functions on coproducts via pattern matching, just as is possible in any functional programming language:
\[
\mathsf{either} (f : L \to A) (g : R \to A) (x : L + R) : A \triangleq
  \mathsf{cases}(x)
  \begin{cases}
  \mathsf{inl}(\ell) \quad \Longrightarrow \quad &f(\ell)
  \\ \mathsf{inr}(r) \quad \Longrightarrow \quad &g(r)
  \end{cases}
\]

We can imagine that this is ``run'' on points in the following way. First, a point $x : L + R$ is presented with the open cover
\[
\top \vdash_{L + R} \mathsf{inl}(L) \vee \mathsf{inr}(R).
\]
Suppose we have that $x$ is actually on the $L$ side, so that we learn $x \models \mathsf{inl}(L)$. Therefore, we now have a point $x' : \{ L + R \suchthat \mathsf{inl}(L) \}$. Since we have proved that $\mathsf{inl}$ is an open embedding, there is a map $\mathsf{inl}^{-1} : \{ L + R \suchthat \mathsf{inl}(L) \} \hookto L$, and so we can define $\ell \triangleq \mathsf{inl}^{-1}(x')$, and then follow the first case branch to compute $f(\ell)$ as the result.

Now, let's define the unique polymorphic map
\[
\mathsf{distr}(x : A \times B + A \times C) : A \triangleq
\mathsf{cases}(x)
  \begin{cases}
  \mathsf{inl}(a, b) \quad \Longrightarrow \quad &a
  \\ \mathsf{inr}(a, c) \quad \Longrightarrow \quad &a
  \end{cases}
\]

Here, we have introduced the new notion that we can pattern match on products. The way that we ``execute'' this is, for instance, looking at the first branch, if we've already computed $\ell : A \times B$ as was previously described, we simply define $a \triangleq \mathsf{fst}(\ell)$ and $b \triangleq \mathsf{snd}(\ell)$. This sort of pattern matching seems to be specific for product spaces.

Additionally, I'll note that sites (or Grothendieck pretopologies\footnote{See \url{https://ncatlab.org/nlab/show/Grothendieck+pretopology}.}?) should have the singleton cover which is the whole space (``reflexivity''), which corresponds to a pattern which may change to an isomorphic space but doesn't really break it apart, as well as a notion of composition of covers (``transitivity''), which corresponds to the potential to nest patterns. For instance, given the homeomorphism $\mathsf{add3} : \mathbb{Z} \hookto \mathbb{Z}$, we can get the inverse map using a pattern,
\begin{align*}
\mathsf{subtract3} &: \mathbb{Z} \to \mathbb{Z}
\\
\mathsf{subtract3}(x) &\triangleq
\mathsf{cases}(x)
\begin{cases}
\mathsf{add3}(y) \Longrightarrow y
\end{cases}.
\end{align*}

Recall that this is not very impressive, since proving that $\mathsf{add3}$ is an open embedding requires providing such an inverse map in the first place.

In general, the covers will not be disjoint, so we need to deal with what happens when different cases overlap. For instance, consider the autonomous car which needs to decide whether to brake as it approaches a yellow light:
\begin{align*}
\mathsf{brake?} &: \R \to \PLower^+(\bool)
\\ \mathsf{brake?}(s) &\triangleq \mathsf{cases}(s)
\begin{cases}
x \models \cdot > - 1
  \quad &\Longrightarrow \quad
  \{ \mathsf{false} \}
\\
y \models \cdot < 0
  \quad &\Longrightarrow \quad
  \{ \mathsf{true} \}
\end{cases}
\end{align*}

The syntax for this style of pattern match is up in the air. We note that we have the cover
\[
\top \vdash_\R \cdot > -1 \vee \cdot < 0,
\]
and we can identify the open $\cdot > -1 : \Open{\R}$ with an open embedding $\{ \R \suchthat \cdot > -1 \} \hookto \R$. The pattern $x \models \cdot > -1$ is supposed to indicate that $x : \{ \R \suchthat \cdot > -1 \}$ is the variable that was ``injected'' into $\R$ by that open embedding. Though in this particular case, the $x$ and $y$ patterns in the case expression aren't even used.

We imagine the same process for computing the result of this expression, in some sense. We first present our input point $s$ with the appropriate cover, and then follow that branch. But what if $s$ satisfies several of the cases? Then it might follow any of the branches. So the behavior of the output should be the union of the behaviors in all possible branches. In this case, if we have $-1 < s < 0$, then $s$ satisfies both cases. Here, it is clear that the right notion of the output point is the subspace $\{ \mathsf{true}, \mathsf{false} \} : \PLower^+(\bool)$. The key property that makes this the ``correct'' answer is that, in terms of specialization order,
\[
\{ \mathsf{true}, \mathsf{false} \} = \max\left( \{ \mathsf{true} \} , \{ \mathsf{false} \} \right).
\]
We note that the key fact is that it is possible to take \emph{directed} suprema of points (as well as continuous maps), so we can specify the output result by saying its behavior is the union (i.e., supremum) of all possible behaviors, and we can create an implementation for this specification by ensuring that this supremum is the \emph{directed} supremum of the behaviors of the each possible branch. We can ensure this by providing, for each pair of branches $f_i : A_i \hookto B$ and $f_j : A_j \hookto B$, an open embedding $f_{ij} : A_i \times_B A_j \hookto B$ from the pullback of $f_i$ and $f_j$ such that
\[
f_{ij} = \max(f_i \circ e_i, f_j \circ e_j),
\]
where $e_i : A_i \times_B A_j \hookto A_i$ and $e_j : A_i \times_B A_j \hookto A_j$ are the expected pullback maps.
In the case where the open embeddings are simply open subspaces, $f_i : \{ B \suchthat U_i \} \hookto B$ and $f_j : \{ B \suchthat U_j \} \hookto B$, then the requirement reduces to producing a map $f_{ij} : \{ B \suchthat U_i \wedge U_j \} \hookto B$ such that $f_{ij} = \max(f_i, f_j)$ in terms of specialization order (considering all these functions on their common domain $ \{ B \suchthat U_i \wedge U_j \}$). While this may seem like a special case, it's just as general: the difference is just up to the homeomorphism between a general space and the open subspace which is the image of the open embedding. That is, even given $f_i : A_i \hookto B$ and $f_j : A_j \hookto B$, it suffices to give a map
\[
f_{ij} : \{B \suchthat f_i(A_i) \wedge f_j(A_j) \} \hookto B,
\]
since
\[
 \{B \suchthat f_i(A_i) \wedge f_j(A_j) \} \cong A_i \times_B A_j.
\]

For the inhabited $\lozenge$-powerspace $\PLower^+(A)$ of any space $A$, this maximum \emph{always} exists: it's just the familiar join operation, which takes two non-deterministic operations and executes either one non-deterministically. Its general specification is
\begin{align*}
\cdot \vee \cdot &: \PLower^+(A) \times \PLower^+(A) \to \PLower^+(A)
\\ x \vee y \models \lozenge P &\triangleq x \models \lozenge P \vee y \models \lozenge P,
\end{align*}
and this definition makes it clear why this produces the maximum in terms of specialization order.

\subsection{General specification of overlapping patterns}
So in general, a general pattern match on a term $x : A$ to produce a value in $B$, might look like
\[
\mathsf{cases}(x)
\begin{cases}
[i : I] \quad f_i(x_i) \quad \Longrightarrow e_i(x_i)
\end{cases},
\]
where $i$ ranges over the index set $I$, and $f_i : A_i \hookto A$ is an open embedding, and $e_i : A_i \to B$ is a general expression, and $x_i : A_i$ is a pattern. The $\mathsf{cases}$ expression above entirely determines the \emph{specification} of the program, assuming it is valid, but we still need to prove two conditions to show that it is valid, which are also then used in the \emph{implementation} of the program. That means that these proofs will affect the \emph{computational} behavior of the program, meaning that different proofs might result in different responses to open covers when points are evaluated. The two conditions are
\begin{description}
\item[Covering]
\[
\top \vdash_A \bigvee_{i : I} f_i(A_i)
\]
\item[Gluing] For each $i, j : I$, there is a map
\[
f_{ij} : A_i \times_A A_j \to B
\]
such that 
\[
f_{ij} = \max(f_i \circ e_i, f_j \circ e_j),
\]
where $e_i : A_i \times_A A_j \hookto A_i$ and $e_j : A_i \times_A A_j \hookto A_j$ are the expected pullback maps.
\end{description}

Sometimes, we can automatically/generically discharge either of these two goals. For instance, if the output space $B$ is such that any two (generalized) points have a maximum in terms of specialization order, then the \emph{gluing} condition is automatically fulfilled. For instance, this holds for any lower powerspace $\PLower(A)$, including the supported subspaces $\PLower^+(A)$.

\subsection{Catch-all cases}

If the output space $B$ has a bottom point $\bot_B : B$, then in some sense we can leave out the \emph{covering} condition as well. We note that for any $\mathsf{cases}$ expression
\[
\mathsf{cases}(x)
\begin{cases}
[i : I] \quad f_i(x_i) \quad \Longrightarrow e_i(x_i)
\end{cases}
\]
which is already valid where the output space $B$ has a bottom point, we can add a catch-all case
\[
\mathsf{cases}(x)
\begin{cases}
[i : I] \quad f_i(x_i) \quad &\Longrightarrow e_i(x_i)
\\ \_\_ \quad &\Longrightarrow \bot_B
\end{cases}
\]
which has the same meaning (where the $\_\_$ wildcard indicates the subspace $\top$ of the input space $A$ which is in fact the entire space). The catch-all case already suffices to cover the whole space, of course. 

Since adding the catch-all case doesn't change the meaning, it makes sense to keep track of which spaces $B$ have bottom points, and when a $\mathsf{cases}$ expression is written whose output space $B$ has a bottom point, the catch-all case can be automatically added.

For any space $A$ there is a construction to produce the ``lifted'' space $A_\bot$ which adds in a bottom point, and in fact, I'm pretty sure that any space $B$ which has a bottom point satisfies $B \cong B_\bot$, which means that \emph{every} space with a bottom point looks like $X_\bot$ for some $X$. For each lifted space $A_\bot$, there is importantly the open embedding $\mathsf{strict} : A \hookto A_\bot$.

We can map a function over lifted spaces with the overlapping pattern
\begin{align*}
\mathsf{map}_\bot(f : A \to B)(x : A_\bot) : B_\bot \triangleq
  \mathsf{cases}(x)
  \begin{cases}
  \mathsf{strict}(z) \quad \Longrightarrow \quad \mathsf{strict}(f(z)),
  \end{cases}
\end{align*}
which I think of as Haskell-ish, as it forces its argument to compute the result, but if the input is $\bot$, then so is the result.

We have that the Sierpínski space $\Sigma$ (the space of ``truth values'') is homeomorphic to $\One_\bot$ (where $\One$ is the unit type), and in fact, it is convenient to take this as the definition of the Sierpínski space, where we have
\begin{align*}
\mathsf{true} &: \Sigma \triangleq \mathsf{strict}(\mathsf{tt})
\\ \mathsf{false} &: \Sigma \triangleq \bot.
\end{align*}

Furthermore, we also notice that the Sierpínski space automatically satisfies the gluing condition, as we also have $\Sigma \cong \PLower(\One)$ (where $\mathsf{false} : \Sigma$ maps to $\{ \} : \PLower(\One)$ and $\mathsf{true} : \Sigma$ maps to $\{ \mathsf{tt} \} : \PLower(\One)$). Therefore, whenever writing any overlapping pattern match which outputs to $\Sigma$, no additional proofs are necessary! Simply writing the specification with the overlapping pattern is sufficient.

This yields some cute ways of writing the logical ``and'' and ``or'' operations for $\Sigma$:
\begin{align*}
\wedge &: \Sigma \times \Sigma \to \Sigma
\\ \wedge(p) &\triangleq \mathsf{cases}(p)
\begin{cases}
\mathsf{strict} , \mathsf{strict}
  \quad \Longrightarrow \quad \mathsf{true}
\end{cases}
\\
\vee &: \Sigma \times \Sigma \to \Sigma
\\ \vee(p) &\triangleq \mathsf{cases}(p)
\begin{cases}
\mathsf{strict} , \_\_
  \quad &\Longrightarrow \quad \mathsf{true}
\\  \_\_ , \mathsf{strict}
  \quad &\Longrightarrow \quad \mathsf{true}
\end{cases}
\end{align*}

The first definition perhaps looks similar to the Haskell definition
\begin{verbatim}
and :: () -> () -> ()
and () () = ()
\end{verbatim}
which looks trivial, but has an important computational interpretation of forcing both of its arguments whenever the return value is forced. However, the definition of $\vee$, which performs the ``or'' operation, has no analog in Haskell. The ``similar'' Haskell definition
\begin{verbatim}
or :: () -> () -> ()
or () _ = ()
or _ () = ()
\end{verbatim}
will in fact behave computationally as if the second pattern were missing (i.e., it forces the first argument but not the second). This operation is often known as the ``parallel-OR'' operation, which Mart\'in Escard\'o discusses in \cite{escardo2004}. It has this name because of the interpretation of the space $\Sigma$ representing semi-decision procedures: if we have two semi-decision procedures, we can create a procedure which halts if and only if either of the component procedures halts by interleaving them in a parallel fashion. Such a general notion of interleaving is unavailable in most programming languages, as far as I'm aware.

Note that, viewing $\Sigma$ as $\One_\bot$, the ``and'' operation readily generalizes to 
\begin{align*}
 \mathsf{pair}_\bot: A_\bot \times B_\bot \to \left( A \times B \right)_\bot
\\ \mathsf{pair}_\bot(p) &\triangleq \mathsf{cases}(p)
\begin{cases}
\mathsf{strict}(x) , \mathsf{strict}(y)
  \quad \Longrightarrow \quad \mathsf{strict}(x, y)
\end{cases}
\end{align*}
but it isn't possible to generalize ``or'' to something that would look like $ \mathsf{either}_\bot: A_\bot \times A_\bot \to A_\bot$. The fact that the ``or'' operation works for the Sierp\'inski space is because $\One$ has only one element, so one cannot tell if they observed the result due to either the left argument or the right argument, while for general $A$, this may not be the case. This is why the Haskell \texttt{or} definition is problematic. If we instead view $\Sigma$ as $\PLower(\One)$, then the ``and'' operation generalizes to intersection, whereas the ``or'' operation generalizes to union. It doesn't seem that writing intersection and union on $\PLower(A)$ can be phrased as overlapping pattern matches, however (unless $A$ is discrete).

%One can view $\PLower(A)$ as the space of maps $f : \Open{A} \to \Sigma$ where $\Open{A}$ has the Scott topology, requiring $f(\bot) = \mathsf{false}$. Measures $\Meas(A)$ on a space then seem like generalizations of this, being the space of maps $\mu : \Open{A} \to \lowerT{[0, \infty]}$ satisfying $\mu(\bot) = 0$ and some other properties.

